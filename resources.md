# Resources

## Nvidia DGX Datasheet
https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/dgx-station/nvidia-dgx-station-datasheet-uk.pdf

## Web Resources
- Book - https://huggingface.co/spaces/nanotron/ultrascale-playbook
- Deepspeed - https://www.deepspeed.ai/docs/config-json/
- Megatron Docs - https://github.com/NVIDIA/Megatron-LM
- torch DDP tutorial - https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html
- torch DDP YT tutorial - https://www.youtube.com/playlist?list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj
- torch FSDP tutorial - https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html
- torch TP tutorial - https://docs.pytorch.org/tutorials/intermediate/TP_tutorial.html
- torch PP tutorial - https://docs.pytorch.org/tutorials/intermediate/pipelining_tutorial.html
- deepspeed tutorials - https://www.deepspeed.ai/tutorials/


## Papers
- https://arxiv.org/pdf/2303.18223 - LLM Survey
- https://arxiv.org/pdf/1909.08053 - Megatron LM 
- https://arxiv.org/pdf/2207.00032 - Deepspeed 
- https://arxiv.org/pdf/1910.02054 - ZeRO 
- https://arxiv.org/pdf/2407.20018 - Distributed Training survey 
- https://arxiv.org/pdf/2104.04473 - Efficient Large Scale training using GPU clusters
- https://arxiv.org/pdf/1805.04170 - Tensor Parallelism (Although some sources say it was introduced in MegaTron LM)
- https://arxiv.org/pdf/1806.03377 - Pipeline Parallelism  
- https://arxiv.org/pdf/2105.13120 - Sequence Parallelism  
- https://arxiv.org/pdf/2304.11277 - PyTorch FSDP 
